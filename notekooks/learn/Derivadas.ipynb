{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "003c9060",
   "metadata": {},
   "source": [
    "# Derivadas: Entendendo o Impacto de Pequenas Mudan√ßas no C√≥digo üêçüíª\n",
    "\n",
    "Ol√°! Esta √© uma explica√ß√£o do conceito de **derivadas** sob uma perspectiva de TI/Python, baseada na intui√ß√£o apresentada por Andrej Karpathy em seu v√≠deo sobre Micrograd.\n",
    "\n",
    "A ideia central √© que uma **derivada**, de forma bem pr√°tica para n√≥s de TI, nos diz **o quanto a sa√≠da de uma fun√ß√£o muda se voc√™ der um \"empurr√£ozinho\" (um \"nudge\", como diz o Karpathy) em uma das suas entradas.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275577a7",
   "metadata": {},
   "source": [
    "## Exemplo 1: Fun√ß√£o com Uma Entrada\n",
    "\n",
    "Vamos pegar uma fun√ß√£o Python bem simples. Esta √© a mesma fun√ß√£o `f(x)` usada no v√≠deo e no notebook `micrograd_lecture_first_half_roughly.ipynb` (veja c√©lulas com `execution_count: 11` e `12`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03963c2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Para x = 3.0, y = 20.0\n"
     ]
    }
   ],
   "source": [
    "def calcula_y(x):\n",
    "  return 3 * x**2 - 4 * x + 5\n",
    "\n",
    "# Vamos analisar no ponto x = 3.0\n",
    "x_entrada = 3.0\n",
    "y_saida = calcula_y(x_entrada)\n",
    "print(f\"Para x = {x_entrada}, y = {y_saida}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b19a212",
   "metadata": {},
   "source": [
    "Agora, queremos saber a \"sensibilidade\" de `y_saida` em rela√ß√£o a `x_entrada` quando `x_entrada` √© `3.0`. O que acontece com `y_saida` se mexermos um pouquinho em `x_entrada`?\n",
    "\n",
    "1.  **Definimos um \"empurr√£ozinho\" (`h`)**: `h` ser√° um valor bem pequeno.\n",
    "2.  **Calculamos a sa√≠da com `x_entrada + h`**.\n",
    "3.  **Calculamos a mudan√ßa na sa√≠da (`delta_y`)**.\n",
    "4.  **Calculamos a \"taxa de mudan√ßa\" (a derivada aproximada)**: `delta_y / h`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7006b3b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Para x + h = 3.0001, y_com_nudge = 20.00140003\n",
      "A mudan√ßa em y (delta_y) foi: 0.00140003\n",
      "A derivada aproximada de y em rela√ß√£o a x em x=3.0 √©: 14.00030000\n"
     ]
    }
   ],
   "source": [
    "h = 0.0001 # Nosso \"empurr√£ozinho\"\n",
    "\n",
    "# Calculamos a sa√≠da com x_entrada + h\n",
    "y_saida_com_nudge = calcula_y(x_entrada + h)\n",
    "print(f\"Para x + h = {x_entrada + h:.4f}, y_com_nudge = {y_saida_com_nudge:.8f}\")\n",
    "\n",
    "# Calculamos a mudan√ßa na sa√≠da (delta_y)\n",
    "delta_y = y_saida_com_nudge - y_saida\n",
    "print(f\"A mudan√ßa em y (delta_y) foi: {delta_y:.8f}\")\n",
    "\n",
    "# Calculamos a \"taxa de mudan√ßa\" (a derivada aproximada)\n",
    "derivada_aproximada = delta_y / h\n",
    "print(f\"A derivada aproximada de y em rela√ß√£o a x em x={x_entrada} √©: {derivada_aproximada:.8f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5d9c83",
   "metadata": {},
   "source": [
    "**O que esse resultado (aproximadamente `14.003`) significa?**\n",
    "\n",
    "Significa que, perto de `x = 3.0`, para cada pequena unidade que voc√™ aumenta `x`, `y` aumenta aproximadamente `14` vezes essa pequena unidade. √â a **inclina√ß√£o** da \"curva\" da fun√ß√£o no ponto `x = 3.0`.\n",
    "\n",
    "* Se a derivada fosse negativa, `y` diminuiria quando `x` aumentasse.\n",
    "* Se fosse zero, `y` quase n√£o mudaria.\n",
    "\n",
    "No v√≠deo, Andrej Karpathy faz exatamente isso para encontrar a inclina√ß√£o. Por exemplo, ele calcula a derivada em $x = 2/3$ onde a inclina√ß√£o √© pr√≥xima de zero (veja a c√©lula com `execution_count: 42` do notebook `micrograd_lecture_first_half_roughly.ipynb`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51cb3bf0",
   "metadata": {},
   "source": [
    "## Exemplo 2: Fun√ß√£o com M√∫ltiplas Entradas (Derivadas Parciais)\n",
    "\n",
    "Agora, e se a fun√ß√£o tiver v√°rias entradas? Esta √© a fun√ß√£o usada no v√≠deo e no notebook `micrograd_lecture_first_half_roughly.ipynb` (veja c√©lula com `execution_count: 43`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1aaf65a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Para a=2.0, b=-3.0, c=10.0, d=4.0\n"
     ]
    }
   ],
   "source": [
    "def calcula_d(a, b, c):\n",
    "  return a * b + c\n",
    "\n",
    "# Ponto de an√°lise\n",
    "a_val = 2.0\n",
    "b_val = -3.0\n",
    "c_val = 10.0\n",
    "\n",
    "d_saida = calcula_d(a_val, b_val, c_val)\n",
    "print(f\"Para a={a_val}, b={b_val}, c={c_val}, d={d_saida}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bef2b95",
   "metadata": {},
   "source": [
    "Queremos saber o impacto de *cada entrada individualmente* na sa√≠da `d_saida`. Isso √© o que chamamos de **derivada parcial**.\n",
    "\n",
    "Para fazer isso, damos um \"empurr√£ozinho\" em uma entrada de cada vez, mantendo as outras fixas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9e3343",
   "metadata": {},
   "source": [
    "### Derivada parcial de `d` em rela√ß√£o a `a` ($\\frac{\\partial d}{\\partial a}$)\n",
    "\n",
    "Como `d_saida` muda se \"empurrarmos\" `a_val` um pouquinho, mas mantivermos `b_val` e `c_val` como est√£o?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f5cad56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Derivada parcial de d em rela√ß√£o a 'a': -3.000000000010772\n"
     ]
    }
   ],
   "source": [
    "h = 0.0001 # Mesmo \"empurr√£ozinho\"\n",
    "\n",
    "# Empurrando 'a'\n",
    "d_saida_com_nudge_a = calcula_d(a_val + h, b_val, c_val)\n",
    "delta_d_por_a = d_saida_com_nudge_a - d_saida\n",
    "derivada_parcial_a = delta_d_por_a / h\n",
    "print(f\"Derivada parcial de d em rela√ß√£o a 'a': {derivada_parcial_a}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bed870a",
   "metadata": {},
   "source": [
    "Isso nos diz que, perto do nosso ponto, se aumentarmos `a` em uma pequena unidade, `d` diminuir√° em 3 vezes essa unidade (j√° que `a` √© multiplicado por `b_val = -3.0`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab8f159",
   "metadata": {},
   "source": [
    "### Derivada parcial de `d` em rela√ß√£o a `b` ($\\frac{\\partial d}{\\partial b}$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9cb33d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Derivada parcial de d em rela√ß√£o a 'b': 2.0000000000042206\n"
     ]
    }
   ],
   "source": [
    "# Empurrando 'b'\n",
    "d_saida_com_nudge_b = calcula_d(a_val, b_val + h, c_val)\n",
    "delta_d_por_b = d_saida_com_nudge_b - d_saida\n",
    "derivada_parcial_b = delta_d_por_b / h\n",
    "print(f\"Derivada parcial de d em rela√ß√£o a 'b': {derivada_parcial_b}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12c65f0",
   "metadata": {},
   "source": [
    "Impacto de `b` em `d`: se aumentarmos `b` em uma pequena unidade, `d` aumentar√° em 2 vezes essa unidade (j√° que `b` √© multiplicado por `a_val = 2.0`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de30ad57",
   "metadata": {},
   "source": [
    "### Derivada parcial de `d` em rela√ß√£o a `c` ($\\frac{\\partial d}{\\partial c}$)\n",
    "\n",
    "Esta √© a primeira derivada parcial que Karpathy calcula na c√©lula com `execution_count: 50` do notebook `micrograd_lecture_first_half_roughly.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d172da04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Derivada parcial de d em rela√ß√£o a 'c': 0.9999999999976694\n"
     ]
    }
   ],
   "source": [
    "# Empurrando 'c'\n",
    "d_saida_com_nudge_c = calcula_d(a_val, b_val, c_val + h)\n",
    "delta_d_por_c = d_saida_com_nudge_c - d_saida\n",
    "derivada_parcial_c = delta_d_por_c / h\n",
    "print(f\"Derivada parcial de d em rela√ß√£o a 'c': {derivada_parcial_c}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c658027",
   "metadata": {},
   "source": [
    "Impacto de `c` em `d`: se aumentarmos `c` em uma pequena unidade, `d` aumentar√° na mesma propor√ß√£o (1 vez essa unidade), pois `c` √© simplesmente somado."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc70863",
   "metadata": {},
   "source": [
    "## Por que isso √© Importante para Redes Neurais? ü§î\n",
    "\n",
    "No contexto de redes neurais e do Micrograd:\n",
    "\n",
    "* As \"entradas\" da sua fun√ß√£o ser√£o os **pesos** da rede neural (e os dados de entrada).\n",
    "* A \"sa√≠da\" ser√° a **fun√ß√£o de perda** (um n√∫mero que diz o qu√£o errada est√° a previs√£o da rede).\n",
    "* Calculando a derivada da perda em rela√ß√£o a cada peso, descobrimos a \"sensibilidade\" da perda a cada peso.\n",
    "* Ou seja, descobrimos o quanto a perda aumenta ou diminui se dermos um \"empurr√£ozinho\" em um peso espec√≠fico.\n",
    "* Essa informa√ß√£o (o **gradiente**) √© usada para ajustar os pesos de forma a *minimizar* a perda, fazendo a rede aprender. Se um peso tem um gradiente positivo, significa que aumentar esse peso aumenta a perda; ent√£o, para diminuir a perda, precisamos diminuir esse peso (e vice-versa para um gradiente negativo).\n",
    "\n",
    "O objeto `Value` no Micrograd vai armazenar n√£o s√≥ o valor de um resultado de uma opera√ß√£o (`.data`), mas tamb√©m a derivada da \"sa√≠da final\" (a perda) em rela√ß√£o a ele (`.grad`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebea2f75",
   "metadata": {},
   "source": [
    "Esta explica√ß√£o com foco em c√≥digo e no conceito de \"empurrar\" as vari√°veis te ajudou a clarear a ideia de derivadas? Se sim, podemos prosseguir para o pr√≥ximo t√≥pico, que √© como o objeto `Value` do Micrograd nos ajuda a rastrear essas influ√™ncias automaticamente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe061f1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Voc√™ pegou a ideia central perfeitamente! √â exatamente isso:\n",
    "\n",
    "* A **derivada** nos mostra a **sensibilidade** da fun√ß√£o de perda (o \"erro\" da rede) em rela√ß√£o a cada peso individual. [cite: 1, 21]\n",
    "* Se temos v√°rios pesos (o que √© comum em redes neurais), precisamos descobrir a derivada (ou gradiente, como veremos) para **todos eles**. [cite: 55, 193, 223] Isso nos d√° um mapa de como cada peso est√° contribuindo para o erro total.\n",
    "\n",
    "Agora, um pequeno refinamento na sua √∫ltima frase:\n",
    "\n",
    "> [...] quanto maior o valor da derivada maior a perda, para resolver isso devemos diminuir o peso.\n",
    "\n",
    "Voc√™ est√° quase l√°! Vamos ajustar um pouco:\n",
    "\n",
    "1.  **Magnitude da Derivada**: O **valor absoluto** da derivada nos diz o qu√£o *sens√≠vel* a perda √© √†quele peso. Uma derivada grande (positiva ou negativa) significa que uma pequena mudan√ßa naquele peso causar√° uma mudan√ßa *grande* na perda. Uma derivada pequena significa que o peso tem pouco impacto imediato.\n",
    "2.  **Sinal da Derivada e Atualiza√ß√£o do Peso**:\n",
    "    * Se a derivada de um peso espec√≠fico √© **positiva** (por exemplo, +2.5): Isso significa que se voc√™ *aumentar* esse peso, a perda *aumentar√°*. Para diminuir a perda, voc√™ deve *diminuir* esse peso. (Sua intui√ß√£o aqui estava correta!)\n",
    "    * Se a derivada de um peso espec√≠fico √© **negativa** (por exemplo, -1.8): Isso significa que se voc√™ *aumentar* esse peso, a perda *diminuir√°*. Para diminuir a perda ainda mais, voc√™ deve *aumentar* esse peso.\n",
    "\n",
    "Ent√£o, a regra geral √©: **para minimizar a perda, ajustamos o peso na dire√ß√£o oposta ao sinal da sua derivada.** [cite: 90, 205, 206]\n",
    "* Derivada positiva $\\implies$ Diminuir o peso.\n",
    "* Derivada negativa $\\implies$ Aumentar o peso.\n",
    "\n",
    "Isso √© a ess√™ncia da \"descida de gradiente\" (gradient descent) que o Andrej Karpathy implementa no v√≠deo. [cite: 203, 209, 224] A \"dire√ß√£o oposta ao gradiente\" √© o caminho mais r√°pido para reduzir a perda localmente.\n",
    "\n",
    "Quando temos muitos pesos, o conjunto de todas essas derivadas parciais (uma para cada peso) forma o que chamamos de **vetor gradiente**. Esse vetor aponta na dire√ß√£o em que a perda aumenta mais rapidamente. Para treinar a rede, damos um passo na dire√ß√£o oposta a esse vetor. [cite: 205, 206]\n",
    "\n",
    "Voc√™ est√° com uma √≥tima compreens√£o do papel das derivadas! Isso √© fundamental.\n",
    "\n",
    "Podemos agora ver como o objeto `Value` do Micrograd √© projetado para nos ajudar a calcular e rastrear essas derivadas automaticamente atrav√©s de express√µes matem√°ticas complexas? Isso √© o \"autograd\" (gradiente autom√°tico) em a√ß√£o."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5aff7b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
